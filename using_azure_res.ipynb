{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeOWTEjoF4kR"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "!pip install gradio tenacity llama-index azure-core azure-search-documents azure-storage-blob openai python-dotenv torch pypdf python-magic-bin\n",
        "\n",
        "# Now import the required libraries\n",
        "import os\n",
        "import gradio as gr\n",
        "import time\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
        "from llama_index.core import SimpleDirectoryReader, Settings, VectorStoreIndex, SummaryIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.azure_openai import AzureOpenAI\n",
        "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set environment variables securely\n",
        "os.environ['AZURE_OPENAI_API_KEY'] = os.getenv('AZURE_OPENAI_API_KEY')\n",
        "os.environ['AZURE_OPENAI_ENDPOINT'] = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
        "os.environ['AZURE_SEARCH_API_KEY'] = os.getenv('AZURE_SEARCH_API_KEY')\n",
        "os.environ['AZURE_SEARCH_ENDPOINT'] = os.getenv('AZURE_SEARCH_ENDPOINT')\n",
        "os.environ['AZURE_STORAGE_CONNECTION_STRING'] = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
        "\n",
        "\n",
        "\n",
        "@retry(\n",
        "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
        "    stop=stop_after_attempt(5),\n",
        "    retry=retry_if_exception_type(Exception)\n",
        ")\n",
        "def chat_with_retry(self, messages, **kwargs):\n",
        "    try:\n",
        "        return self._chat(messages, **kwargs)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}. Retrying...\")\n",
        "        raise\n",
        "\n",
        "# Replace the original chat method with the retry version\n",
        "AzureOpenAI.chat = chat_with_retry\n",
        "\n",
        "def get_router_query_engine(file_paths: list, model_deployment: str):\n",
        "    llm = AzureOpenAI(\n",
        "        model=model_deployment,\n",
        "        deployment_name=model_deployment,\n",
        "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "    )\n",
        "    Settings.llm = llm\n",
        "    Settings.embed_model = AzureOpenAIEmbedding(\n",
        "        model=\"text-embedding-ada-002\",\n",
        "        deployment_name=\"text-embedding-ada-002\",\n",
        "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "    )\n",
        "\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "\n",
        "    query_engine_tools = []\n",
        "    for i, file_path in enumerate(file_paths, 1):\n",
        "        documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "        nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "        summary_index = SummaryIndex(nodes)\n",
        "        vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "        summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\", use_async=True)\n",
        "        vector_query_engine = vector_index.as_query_engine()\n",
        "\n",
        "        summary_tool = QueryEngineTool.from_defaults(\n",
        "            query_engine=summary_query_engine,\n",
        "            description=f\"Useful for summarization questions related to document {i}\"\n",
        "        )\n",
        "        vector_tool = QueryEngineTool.from_defaults(\n",
        "            query_engine=vector_query_engine,\n",
        "            description=f\"Useful for retrieving specific context from document {i}\"\n",
        "        )\n",
        "\n",
        "        query_engine_tools.extend([summary_tool, vector_tool])\n",
        "\n",
        "    router_query_engine = RouterQueryEngine(\n",
        "        selector=LLMSingleSelector.from_defaults(),\n",
        "        query_engine_tools=query_engine_tools,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    return router_query_engine\n",
        "\n",
        "def process_query(files, model_deployment, query):\n",
        "    if not files:\n",
        "        return \"Please upload at least one PDF file.\"\n",
        "\n",
        "    file_paths = [file.name for file in files]\n",
        "    query_engine = get_router_query_engine(file_paths, model_deployment)\n",
        "\n",
        "    try:\n",
        "        response = query_engine.query(query)\n",
        "        return str(response)\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_query,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload PDFs\", file_count=\"multiple\"),\n",
        "        gr.Dropdown(\n",
        "            choices=[\n",
        "                \"gpt-35-turbo\",\n",
        "                \"gpt-35-turbo-16k\",\n",
        "                \"gpt-4\",\n",
        "                \"gpt-4-32k\"\n",
        "            ],\n",
        "            label=\"Choose Azure OpenAI Model Deployment\"\n",
        "        ),\n",
        "        gr.Textbox(label=\"Enter your query\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Response\"),\n",
        "    title=\"Agentic RAG with Azure Services\",\n",
        "    description=\"Upload multiple PDFs, select an Azure OpenAI model deployment, and ask questions about specific documents or all documents also can summarize the documents.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(share=True)"
      ]
    }
  ]
}